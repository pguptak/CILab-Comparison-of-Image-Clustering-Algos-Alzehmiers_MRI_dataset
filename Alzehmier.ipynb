{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab71228-a3b2-43d3-97a7-f5009fba681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atish\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import (\n",
    "    ResNet152, MobileNetV3Large, VGG19, Xception\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90432fc-ec4b-43df-9218-bcc11583aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset unzipped completely\n"
     ]
    }
   ],
   "source": [
    "ZIP_PATH = \"alzehmier.zip\"   # change if needed\n",
    "\n",
    "WORKING_DIR = \"ALZ_working\"\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"dataset\")\n",
    "MIXED_DIR = os.path.join(WORKING_DIR, \"mixed_images\")\n",
    "\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "os.makedirs(MIXED_DIR, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(WORKING_DIR)\n",
    "\n",
    "print(\"‚úÖ Dataset unzipped completely\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caed6d3-5964-414f-86ab-38d760e07593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Dataset root detected: ALZ_working\\dataset\n",
      "‚û°Ô∏è Processing class: Mild_Demented\n",
      "‚û°Ô∏è Processing class: Moderate_Demented\n",
      "‚û°Ô∏è Processing class: Non_Demented\n",
      "‚û°Ô∏è Processing class: Very_Mild_Demented\n",
      "‚úÖ Mixed images created: 6400\n",
      "‚úÖ labels.csv saved successfully\n"
     ]
    }
   ],
   "source": [
    "image_records = []\n",
    "image_counter = 0\n",
    "\n",
    "dataset_root = DATA_DIR\n",
    "print(\"üìÇ Dataset root detected:\", dataset_root)\n",
    "\n",
    "for class_name in sorted(os.listdir(dataset_root)):\n",
    "    class_path = os.path.join(dataset_root, class_name)\n",
    "\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"‚û°Ô∏è Processing class: {class_name}\")\n",
    "\n",
    "    for image_name in sorted(os.listdir(class_path)):\n",
    "        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "\n",
    "            image_counter += 1\n",
    "            new_image_name = f\"img_{image_counter:06d}.png\"\n",
    "\n",
    "            src_path = os.path.join(class_path, image_name)\n",
    "            dst_path = os.path.join(MIXED_DIR, new_image_name)\n",
    "\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "            image_records.append({\n",
    "                \"image_name\": new_image_name,\n",
    "                \"label\": class_name\n",
    "            })\n",
    "\n",
    "print(f\"‚úÖ Mixed images created: {image_counter}\")\n",
    "\n",
    "labels_df = pd.DataFrame(image_records)\n",
    "LABELS_CSV = os.path.join(WORKING_DIR, \"labels.csv\")\n",
    "labels_df.to_csv(LABELS_CSV, index=False)\n",
    "\n",
    "print(\"‚úÖ labels.csv saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8272d2e6-e480-4d4e-8cd8-f9f3077e9c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6400 validated image filenames belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "generator = datagen.flow_from_dataframe(\n",
    "    dataframe=labels_df,\n",
    "    directory=MIXED_DIR,\n",
    "    x_col=\"image_name\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5156d56a-55b3-41bf-a606-d990a5b16e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Class Weights: {0: np.float64(0.5), 1: np.float64(0.7142857142857143), 2: np.float64(1.7857142857142858), 3: np.float64(25.0)}\n"
     ]
    }
   ],
   "source": [
    "class_counts = labels_df['label'].value_counts()\n",
    "total = class_counts.sum()\n",
    "\n",
    "class_weights = {\n",
    "    i: total / (len(class_counts) * count)\n",
    "    for i, count in enumerate(class_counts)\n",
    "}\n",
    "\n",
    "print(\"‚öñÔ∏è Class Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbb2c36-123e-4b0d-96f0-de50a0f8200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, num_classes):\n",
    "    base = base_model(\n",
    "        include_top=False,\n",
    "        weights=None,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base.input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525cae6-a970-4b4b-b880-eac679ddf091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "FEATURE_DIR = \"saved_features\"\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "\n",
    "true_labels = labels_df['label'].astype('category').cat.codes.values\n",
    "NUM_CLASSES = labels_df['label'].nunique()\n",
    "\n",
    "print(\"‚úÖ Common setup completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a28756",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = {}\n",
    "generator.shuffle = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee42d2-cd3d-4a95-bf1f-f8186f7e789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training ResNet152\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüöÄ Training EfficientNetB0\")\n",
    "\n",
    "efficientnet = build_model(tf.keras.applications.EfficientNetB0, NUM_CLASSES)\n",
    "\n",
    "efficientnet.fit(\n",
    "    generator,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "efficientnet_fe = Model(\n",
    "    inputs=efficientnet.input,\n",
    "    outputs=efficientnet.layers[-3].output\n",
    ")\n",
    "\n",
    "efficientnet_features = efficientnet_fe.predict(generator)\n",
    "\n",
    "np.save(\n",
    "    os.path.join(FEATURE_DIR, \"EfficientNetB0_features.npy\"),\n",
    "    efficientnet_features\n",
    ")\n",
    "\n",
    "feature_results[\"EfficientNetB0\"] = efficientnet_features\n",
    "\n",
    "print(\"üíæ EfficientNetB0 features saved:\", efficientnet_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Training ResNet152\")\n",
    "\n",
    "resnet = build_model(tf.keras.applications.ResNet152, NUM_CLASSES)\n",
    "\n",
    "resnet.fit(\n",
    "    generator,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "resnet_fe = Model(\n",
    "    inputs=resnet.input,\n",
    "    outputs=resnet.layers[-3].output\n",
    ")\n",
    "\n",
    "resnet_features = resnet_fe.predict(generator)\n",
    "\n",
    "np.save(\n",
    "    os.path.join(FEATURE_DIR, \"ResNet152_features.npy\"),\n",
    "    resnet_features\n",
    ")\n",
    "\n",
    "feature_results[\"ResNet152\"] = resnet_features\n",
    "\n",
    "print(\"üíæ ResNet152 features saved:\", resnet_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4394f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Training MobileNetV3\")\n",
    "\n",
    "mobilenet = build_model(\n",
    "    tf.keras.applications.MobileNetV3Large,\n",
    "    NUM_CLASSES\n",
    ")\n",
    "\n",
    "mobilenet.fit(\n",
    "    generator,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mobilenet_fe = Model(\n",
    "    inputs=mobilenet.input,\n",
    "    outputs=mobilenet.layers[-3].output\n",
    ")\n",
    "\n",
    "mobilenet_features = mobilenet_fe.predict(generator)\n",
    "\n",
    "np.save(\n",
    "    os.path.join(FEATURE_DIR, \"MobileNetV3_features.npy\"),\n",
    "    mobilenet_features\n",
    ")\n",
    "\n",
    "print(\"üíæ MobileNetV3 features saved:\", mobilenet_features.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ae289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Training VGG19\")\n",
    "\n",
    "vgg = build_model(tf.keras.applications.VGG19, NUM_CLASSES)\n",
    "\n",
    "vgg.fit(\n",
    "    generator,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "vgg_fe = Model(\n",
    "    inputs=vgg.input,\n",
    "    outputs=vgg.layers[-3].output\n",
    ")\n",
    "\n",
    "vgg_features = vgg_fe.predict(generator)\n",
    "\n",
    "np.save(\n",
    "    os.path.join(FEATURE_DIR, \"VGG19_features.npy\"),\n",
    "    vgg_features\n",
    ")\n",
    "\n",
    "print(\"üíæ VGG19 features saved:\", vgg_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Training Xception\")\n",
    "\n",
    "xception = build_model(tf.keras.applications.Xception, NUM_CLASSES)\n",
    "\n",
    "xception.fit(\n",
    "    generator,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xception_fe = Model(\n",
    "    inputs=xception.input,\n",
    "    outputs=xception.layers[-3].output\n",
    ")\n",
    "\n",
    "xception_features = xception_fe.predict(generator)\n",
    "\n",
    "np.save(\n",
    "    os.path.join(FEATURE_DIR, \"Xception_features.npy\"),\n",
    "    xception_features\n",
    ")\n",
    "\n",
    "print(\"üíæ Xception features saved:\", xception_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(FEATURE_DIR, \"true_labels.npy\"), true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3440b-1893-4014-af6d-514c5b8a9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model_name, features in feature_results.items():\n",
    "    print(f\"\\nüîç Clustering for {model_name}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    pca = PCA(n_components=0.95)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=NUM_CLASSES, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(features_pca)\n",
    "\n",
    "    changed = np.sum(cluster_labels != true_labels)\n",
    "    change_percentage = (changed / len(true_labels)) * 100\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Original Samples\": len(true_labels),\n",
    "        \"Changed Labels\": changed,\n",
    "        \"Change %\": round(change_percentage, 2)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e1ec2-c6f7-4466-93b4-1bf1a6eb4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä FINAL CLUSTERING RESULTS\")\n",
    "display(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
